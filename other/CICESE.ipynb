{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fa4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f340309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_elapsed(function, *args):\n",
    "    start = time.perf_counter()\n",
    "    function(*args)\n",
    "    end = time.perf_counter()\n",
    "    print(f\"Time elapsed: {end - start:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a7bea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gather_cicese_data(year_from, directory_to, location=\"isla_cedros\"):\n",
    "    location_dict = {\"isla_cedros\":\"ICDN\", \"guerrero_negro\":\"GRON\"}\n",
    "    \n",
    "    # Column names obtained from CICESE files metadata. None of this files have a header\n",
    "    columns=[\"anio\",\"mes\",\"dia\",\"hora\",\"minuto\",\"segundo\",\n",
    "             \"id_estacion\",\"voltaje_sistema\",\"nivel_mar_leveltrol\",\"nivel_mar_burbujeador\",\n",
    "             \"sw_1\",\"sw_2\",\"temperatura_agua\",\"nivel_mar_ott_rsl\", \"radiacion_solar\",\n",
    "             \"direccion_viento\", \"magnitud_viento\", \"temperatura_aire\",\"humedad_relativa\",\n",
    "             \"presion_atmosferica\",\"precipitacion\",\"voltaje_estacion_met\",\"nivel_mar_sutron\"]\n",
    "\n",
    "    # df is the dataframe that will allocate all the data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # We set the directory where we will download the data\n",
    "    directory_path = directory_to + location\n",
    "    if not os.path.isdir(directory_path):\n",
    "        os.mkdir(directory_path)\n",
    "\n",
    "    os.chdir(directory_path) #changes the active dir - this is where downloaded files will be saved to\n",
    "    \n",
    "    # We have data from 2011 to 2021. \n",
    "    years = list(range(year_from, datetime.now().year+1))\n",
    "    for year in years:\n",
    "    \n",
    "        # Define the URL of the directory containing the .dat files\n",
    "        url = \"http://redmar.cicese.mx/emmc/DATA/\"+location_dict[location]+\"/MIN/\"+str(year)+\"/\"\n",
    "\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Extract the HTML content of the response\n",
    "        html_content = response.content.decode('utf-8')\n",
    "\n",
    "        # Find all the .dat file names in the HTML content\n",
    "        dat_files = []\n",
    "        for line in html_content.split('\\n'):\n",
    "            if '.dat' in line:\n",
    "                filename = line.split('href=\"')[1][:15]\n",
    "                dat_files.append(filename)\n",
    "    \n",
    "\n",
    "        # Download each .dat file and save it in the data directory\n",
    "        for filename in dat_files:\n",
    "            try:\n",
    "                file_url = url + filename\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                response = requests.get(file_url)\n",
    "                \n",
    "                if not os.path.exists(file_path):\n",
    "                    with open(file_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "\n",
    "\n",
    "                    # Open the downloaded file and read its content\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        content = f.read()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(filename, \"no se agreg√≥ por: \", e)\n",
    "                \n",
    "        # Rename df columns with the ones defined before\n",
    "        dict_columns = {}\n",
    "        for col, i in zip(columns, range(len(columns))):\n",
    "            dict_columns[i] = col\n",
    "        dict_columns\n",
    "        df = df.rename(columns=dict_columns)\n",
    "\n",
    "        # Export csv\n",
    "        df.to_csv(str(year_from)+\"_\"+location+\".csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32427182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cicese_data(place):\n",
    "    columns=[\"anio\",\"mes\",\"dia\",\"hora\",\"minuto\",\"segundo\",\n",
    "             \"id_estacion\",\"voltaje_sistema\",\"nivel_mar_leveltrol\",\"nivel_mar_burbujeador\",\n",
    "             \"sw_1\",\"sw_2\",\"temperatura_agua\",\"nivel_mar_ott_rsl\", \"radiacion_solar\",\n",
    "             \"direccion_viento\", \"magnitud_viento\", \"temperatura_aire\",\"humedad_relativa\",\n",
    "             \"presion_atmosferica\",\"precipitacion\",\"voltaje_estacion_met\",\"nivel_mar_sutron\"]\n",
    "    # Set the directory path\n",
    "    dir_path = 'data/'+place\n",
    "    \n",
    "    # Get a list of all .dat files in the directory\n",
    "    dat_files = glob.glob(os.path.join(dir_path, \"*.dat\"))\n",
    "\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through each file and read it into a dataframe\n",
    "    for file in dat_files:\n",
    "        df = pd.read_csv(file, lineterminator='\\n', delim_whitespace=True, header=None)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all the dataframes into a single dataframe\n",
    "    result_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    \n",
    "    # Rename df columns with the ones defined before\n",
    "    dict_columns = {}\n",
    "    for col, i in zip(columns, range(len(columns))):\n",
    "        dict_columns[i] = col\n",
    "    dict_columns\n",
    "    result_df = result_df.rename(columns=dict_columns)\n",
    "\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae065079",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_to = \"C:\\\\Users\\\\javi2\\\\Documents\\\\CD_aplicada_1\\\\COBI\\\\data\\\\cicese\\\\\"\n",
    "gather_cicese_data(2021, directory_to=directory_to, location=\"guerrero_negro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e855723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_cicese_data(\"isla_cedros\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8d28276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Runs every time we get new sensors data from cicese:\")\n",
    "# print(time_elapsed(gather_cicese_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e530a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/mod/data_cicese_isla_cedros.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a75504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6218ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
